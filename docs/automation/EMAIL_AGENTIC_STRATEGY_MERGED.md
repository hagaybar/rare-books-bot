# Email Agentic Strategy - Merged Implementation Plan

**Date:** 2025-11-20
**Status:** âœ… Phase 1 Complete | ğŸ“‹ Phase 2-4 Ready
**Branch:** `feature/email-agentic-strategy`
**Version:** 2.0 (Merged with Quality Enhancements)

**Phase 1 Completion:** See [PHASE1_COMPLETION.md](PHASE1_COMPLETION.md) for details

---

## Executive Summary

This plan merges the **agentic orchestration approach** with **advanced quality enhancements** to create a production-grade email RAG system. The key insight: email retrieval requires not just better search strategies, but also **sophisticated context assembly** and **answer validation** to deliver high-quality responses.

**Core Philosophy:**
1. **Intent-driven retrieval** - Different queries need different strategies
2. **Clean context assembly** - Remove noise, deduplicate, organize chronologically
3. **Quality validation** - Verify answers for accuracy and completeness
4. **Incremental delivery** - Quick wins first, then advanced features

---

## Current Limitations (What We're Fixing)

### âŒ Retrieval Problems
- **Fixed K=5** - Not enough for email threads
- **No thread awareness** - Fragmented conversations
- **No temporal ordering** - Random chronological order
- **No sender filtering** - Can't answer "What did X say?"

### âŒ Context Quality Problems (Critical Gap!)
- **Quoted text duplication** - Email replies include previous messages
- **Signature noise** - Every email has boilerplate
- **No deduplication** - Same content repeated 3-5 times in threads
- **Token waste** - 70%+ of context is redundant

### âŒ Answer Quality Problems
- **No validation** - Can't detect contradictions or unsupported claims
- **Generic prompts** - Same prompt for all query types
- **No re-ranking** - First-pass retrieval might miss best results
- **Noise included** - Newsletters, auto-replies mixed with real emails

---

## Key Enhancements

### ğŸ¯ Critical (Phase 1-2)

**1. Context Assembler & Refiner** â­ **MOST CRITICAL**
- **Quote/reply deduplication** - Strip `>` quoted text and "On X wrote:" blocks
- **Signature removal** - Remove email signatures and disclaimers
- **Chronological merging** - Order thread emails properly
- **Source attribution** - Clear "From: X, Date: Y" headers
- **Result:** Clean, coherent conversation context instead of redundant noise

**2. Thread Reconstruction**
- Complete thread retrieval (all emails in conversation)
- Subject normalization ("Re: X" â†’ "X")
- Chronological ordering
- **Result:** Full conversation context, not fragments

**3. Multi-Aspect Retrieval**
- Combined strategies for complex queries
- Example: "What did Alice say about X last week?"
  â†’ Sender filter + Temporal filter + Semantic search
- **Result:** Better handling of real-world queries

**4. Intent Detection with Multi-Metadata**
- Detect multiple intent signals in one query
- Extract sender, time range, topic from query
- **Result:** More precise strategy selection

### ğŸ”§ Important (Phase 3-4)

**5. Answer Validator**
- Post-processing quality check
- Detect contradictions and unsupported claims
- Verify required elements present
- **Result:** Factually accurate, complete answers

**6. Enhanced Retrieval Ranking**
- Secondary re-ranking with cross-encoder
- Noise filtering (newsletters, auto-replies)
- Dynamic K based on query specificity
- **Result:** Most relevant content prioritized

**7. Intent-Aligned Prompting**
- Specialized prompts per intent type
- Thread summary â†’ "Summarize with key points"
- Action items â†’ "Extract and list tasks/deadlines"
- **Result:** Answers formatted appropriately for query type

**8. Specialized Extractors**
- Action Item Extractor (LLM-based)
- Decision Tracker (pattern + LLM)
- **Result:** Direct answers for specialized queries

---

## Improved Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Email Orchestrator Agent (Enhanced)            â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Intent Detector  â”‚â”€â”€â”€â”€â”€â”€â”‚ Strategy Selector  â”‚         â”‚
â”‚  â”‚  (Multi-aspect)  â”‚      â”‚  (Can combine)     â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚           â”‚                            â”‚                    â”‚
â”‚           â”‚ (LLM fallback              â”‚                    â”‚
â”‚           â”‚  if confidence<0.6)        â”‚                    â”‚
â”‚           â–¼                            â–¼                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚    â”‚      Retrieval Strategy Router           â”‚            â”‚
â”‚    â”‚  (Can execute combined strategies)       â”‚            â”‚
â”‚    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚        â”‚       â”‚        â”‚         â”‚                         â”‚
â”‚        â–¼       â–¼        â–¼         â–¼                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   â”‚Thread  â”‚ â”‚Tempâ”‚ â”‚Senderâ”‚ â”‚Action/  â”‚                  â”‚
â”‚   â”‚Retrieverâ”‚ â”‚oralâ”‚ â”‚Filterâ”‚ â”‚Decision â”‚                  â”‚
â”‚   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                  â”‚
â”‚       â”‚        â”‚       â”‚          â”‚                         â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                    â–¼                                         â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚       â”‚ **Context Assembler & Refiner** â”‚                 â”‚
â”‚       â”‚  â­ NEW CRITICAL COMPONENT       â”‚                 â”‚
â”‚       â”‚                                  â”‚                 â”‚
â”‚       â”‚ 1. Merge threads chronologically â”‚                 â”‚
â”‚       â”‚ 2. Remove quoted text/duplicates â”‚                 â”‚
â”‚       â”‚ 3. Strip signatures/boilerplate  â”‚                 â”‚
â”‚       â”‚ 4. Add source attributions       â”‚                 â”‚
â”‚       â”‚ 5. Summarize if too large        â”‚                 â”‚
â”‚       â”‚ 6. Re-rank by relevance          â”‚                 â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                      â–¼                                       â”‚
â”‚            Assembled Clean Context                          â”‚
â”‚                      â”‚                                       â”‚
â”‚                      â–¼                                       â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚       â”‚   LLM Answer Generator       â”‚                     â”‚
â”‚       â”‚ (Intent-aligned prompt)      â”‚                     â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                      â”‚                                       â”‚
â”‚                      â–¼                                       â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚       â”‚  Answer Validator (QA)       â”‚                     â”‚
â”‚       â”‚  â­ NEW QUALITY CHECK         â”‚                     â”‚
â”‚       â”‚                               â”‚                     â”‚
â”‚       â”‚ - Check for contradictions   â”‚                     â”‚
â”‚       â”‚ - Verify citations           â”‚                     â”‚
â”‚       â”‚ - Flag unsupported claims    â”‚                     â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                      â”‚                                       â”‚
â”‚                      â–¼                                       â”‚
â”‚              Final Validated Answer                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What's New:**
1. **Intent Detector** - Multi-aspect detection, optional LLM fallback
2. **Strategy Selector** - Can combine multiple strategies
3. **Context Assembler** - Critical new component for clean context
4. **Answer Validator** - Quality assurance layer
5. **Specialized Extractors** - Action items and decisions
6. **Re-ranking** - Within Context Assembler

---

## Implementation Phases (Merged Roadmap)

### Phase 1: Foundation with Context Assembly (Week 1) - 8-10 hours

**Priority: ğŸ”´ CRITICAL - Cannot skip**

#### 1.1 Intent Detection (4 hours)

**Pattern-Based Classifier:**
```python
class EmailIntentDetector:
    """Detects user intent from email queries with multi-aspect support."""

    def __init__(self):
        self.patterns = {
            "thread_summary": [
                r"summarize.*discussion",
                r"what.*conversation",
                r"thread about",
                r"exchange about"
            ],
            "sender_query": [
                r"what did (\w+) say",
                r"(\w+)'s (opinion|view|response)",
                r"emails from (\w+)"
            ],
            "temporal_query": [
                r"recent", r"latest", r"last (week|month|day)",
                r"yesterday", r"this (week|month)"
            ],
            "action_items": [
                r"action items", r"tasks", r"deadlines", r"todo"
            ],
            "decision_tracking": [
                r"what was decided", r"final decision",
                r"agreed (on|to)", r"conclusion"
            ]
        }

    def detect(self, query: str) -> dict:
        """
        Detect intent with multi-aspect metadata extraction.

        Returns:
            {
                "primary_intent": "sender_query",
                "confidence": 0.85,
                "metadata": {
                    "sender": "Alice",
                    "time_range": "last_week",
                    "topic_keywords": ["budget"]
                },
                "secondary_signals": ["temporal_query"]  # If multiple intents
            }
        """
        # Pattern matching for all intents
        intent_scores = self._score_patterns(query)

        # Extract metadata (sender names, time expressions, etc.)
        metadata = self._extract_metadata(query)

        # Detect secondary signals
        secondary = [intent for intent, score in intent_scores.items()
                     if score > 0.3 and intent != primary]

        return {
            "primary_intent": max(intent_scores, key=intent_scores.get),
            "confidence": max(intent_scores.values()),
            "metadata": metadata,
            "secondary_signals": secondary
        }

    def _extract_metadata(self, query: str) -> dict:
        """Extract sender names, time ranges, topics from query."""
        metadata = {}

        # Extract sender name
        sender_match = re.search(r"(?:from|by|what did|did)\s+(\w+)", query, re.I)
        if sender_match:
            metadata["sender"] = sender_match.group(1)

        # Extract time range
        if re.search(r"last week", query, re.I):
            metadata["time_range"] = "last_week"
        elif re.search(r"yesterday", query, re.I):
            metadata["time_range"] = "yesterday"
        elif re.search(r"recent|latest", query, re.I):
            metadata["time_range"] = "recent"

        return metadata
```

**Tests:**
```python
def test_multi_aspect_detection():
    detector = EmailIntentDetector()

    # Complex query with multiple aspects
    result = detector.detect("What did Alice say about the budget last week?")

    assert result["primary_intent"] == "sender_query"
    assert "temporal_query" in result["secondary_signals"]
    assert result["metadata"]["sender"] == "Alice"
    assert result["metadata"]["time_range"] == "last_week"
```

**Deliverables:**
- âœ… `scripts/agents/email_intent_detector.py`
- âœ… Multi-aspect metadata extraction
- âœ… Unit tests with 15+ test cases
- âœ… Sample query dataset (30 labeled queries)

---

#### 1.2 Context Assembler MVP (4-6 hours) â­ **CRITICAL**

**Quote/Reply Deduplication:**
```python
class ContextAssembler:
    """
    Assembles clean, organized context from retrieved email chunks.

    Critical responsibilities:
    1. Remove quoted text and reply chains
    2. Strip email signatures
    3. Merge thread emails chronologically
    4. Add clear source attributions
    5. Deduplicate redundant content
    """

    def __init__(self):
        # Common quote patterns
        self.quote_patterns = [
            r'^>+.*$',  # Lines starting with >
            r'On .+ wrote:.*$',  # "On Jan 5, John wrote:"
            r'From:.*Sent:.*To:.*Subject:',  # Email headers in replies
            r'-{3,}.*Original Message.*-{3,}',  # Outlook quote markers
        ]

        # Common signature patterns
        self.signature_patterns = [
            r'^--\s*$',  # Standard signature delimiter
            r'Sent from my (iPhone|iPad|Android)',
            r'Best regards,.*$',
            r'Thanks,.*$',
            r'^_{3,}$',  # Underline separators
        ]

    def assemble(self, chunks: List[Chunk], intent: dict) -> str:
        """
        Assemble clean context from chunks.

        Args:
            chunks: Retrieved email chunks
            intent: Detected intent (for ordering/emphasis)

        Returns:
            Clean, organized context string
        """
        # Step 1: Group by thread if needed
        if intent.get("primary_intent") == "thread_summary":
            threads = self._group_by_thread(chunks)
            chunks = self._merge_threads_chronologically(threads)
        else:
            # Sort by relevance or date based on intent
            chunks = self._sort_chunks(chunks, intent)

        # Step 2: Clean each email
        cleaned_chunks = []
        for chunk in chunks:
            cleaned_text = self._clean_email(chunk.text)

            # Skip if email is now empty (was all quoted text)
            if cleaned_text.strip():
                cleaned_chunks.append({
                    "text": cleaned_text,
                    "meta": chunk.meta
                })

        # Step 3: Deduplicate content across emails
        unique_chunks = self._deduplicate_content(cleaned_chunks)

        # Step 4: Format with source attributions
        context_parts = []
        for i, chunk in enumerate(unique_chunks):
            meta = chunk["meta"]

            # Create attribution header
            sender = meta.get("sender_name", "Unknown")
            subject = meta.get("subject", "No Subject")
            date = meta.get("date", "Unknown Date")

            header = f"Email #{i+1}:\nFrom: {sender}\nSubject: {subject}\nDate: {date}\n"
            context_parts.append(header + "\n" + chunk["text"])

        # Step 5: Join with clear separators
        return "\n\n---\n\n".join(context_parts)

    def _clean_email(self, text: str) -> str:
        """Remove quoted text and signatures from email body."""
        lines = text.split('\n')
        cleaned_lines = []
        in_signature = False

        for line in lines:
            # Check for signature start
            if any(re.match(pattern, line.strip())
                   for pattern in self.signature_patterns):
                in_signature = True
                continue

            # Skip if in signature section
            if in_signature:
                continue

            # Check for quoted text
            is_quote = any(re.match(pattern, line.strip())
                          for pattern in self.quote_patterns)

            if not is_quote:
                cleaned_lines.append(line)

        return '\n'.join(cleaned_lines)

    def _deduplicate_content(self, chunks: List[dict]) -> List[dict]:
        """
        Remove duplicate content across email chunks.

        Uses sliding window to detect repeated text blocks.
        """
        if len(chunks) <= 1:
            return chunks

        unique_chunks = [chunks[0]]  # Keep first email

        for i in range(1, len(chunks)):
            current_text = chunks[i]["text"]

            # Check if current text is largely contained in previous emails
            is_duplicate = False
            for prev_chunk in unique_chunks:
                prev_text = prev_chunk["text"]

                # Simple check: if 80% of current text exists in previous
                similarity = self._text_overlap_ratio(current_text, prev_text)
                if similarity > 0.8:
                    is_duplicate = True
                    break

            if not is_duplicate:
                unique_chunks.append(chunks[i])

        return unique_chunks

    def _text_overlap_ratio(self, text1: str, text2: str) -> float:
        """Calculate what fraction of text1 appears in text2."""
        # Simple word-based overlap
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1:
            return 0.0

        overlap = len(words1 & words2)
        return overlap / len(words1)

    def _group_by_thread(self, chunks: List[Chunk]) -> dict:
        """Group emails by normalized subject (thread ID)."""
        threads = {}

        for chunk in chunks:
            subject = chunk.meta.get("subject", "")
            thread_id = self._normalize_subject(subject)

            if thread_id not in threads:
                threads[thread_id] = []
            threads[thread_id].append(chunk)

        return threads

    def _normalize_subject(self, subject: str) -> str:
        """
        Normalize email subject for thread grouping.

        Examples:
            "Budget Discussion" â†’ "budget discussion"
            "Re: Budget Discussion" â†’ "budget discussion"
            "Fwd: Re: Budget Discussion" â†’ "budget discussion"
        """
        # Remove Re:, Fwd:, etc.
        normalized = re.sub(r'^(re:|fwd?:|fw:)\s*', '', subject, flags=re.I)
        return normalized.lower().strip()

    def _merge_threads_chronologically(self, threads: dict) -> List[Chunk]:
        """
        Merge threads and sort chronologically.

        For thread summaries, we want full conversations in order.
        """
        all_chunks = []

        # Sort threads by relevance (number of chunks)
        sorted_threads = sorted(threads.items(),
                               key=lambda x: len(x[1]),
                               reverse=True)

        # Take top 2-3 threads
        for thread_id, chunks in sorted_threads[:3]:
            # Sort this thread chronologically
            sorted_chunks = sorted(chunks,
                                  key=lambda c: c.meta.get("date", ""))
            all_chunks.extend(sorted_chunks)

        return all_chunks

    def _sort_chunks(self, chunks: List[Chunk], intent: dict) -> List[Chunk]:
        """Sort chunks based on intent (recency vs relevance)."""
        if "temporal" in intent.get("secondary_signals", []):
            # Sort by date (newest first)
            return sorted(chunks,
                         key=lambda c: c.meta.get("date", ""),
                         reverse=True)
        else:
            # Keep relevance order (from semantic search)
            return chunks
```

**Example Output:**

**Before Context Assembler:**
```
Email 1: "Let's meet Tuesday to discuss the budget. Best regards, Alice"
Email 2: "> Let's meet Tuesday to discuss the budget.\nI agree! What time? -Bob\n\nSent from my iPhone"
Email 3: ">> Let's meet Tuesday...\n> I agree! What time?\n2pm works for me. Thanks, Alice"
```
âŒ **Problem:** "Tuesday" appears 3 times, signatures included, hard to follow

**After Context Assembler:**
```
Email #1:
From: Alice
Subject: Budget Discussion
Date: 2025-01-15 09:00:00

Let's meet Tuesday to discuss the budget.

---

Email #2:
From: Bob
Subject: Re: Budget Discussion
Date: 2025-01-15 09:15:00

I agree! What time?

---

Email #3:
From: Alice
Subject: Re: Budget Discussion
Date: 2025-01-15 09:20:00

2pm works for me.
```
âœ… **Better:** Clean conversation, chronological, no duplication!

**Tests:**
```python
def test_quote_removal():
    assembler = ContextAssembler()

    email_with_quotes = """I have a new idea.

> On Jan 5, Bob wrote:
> Let's meet Tuesday.

What do you think?"""

    cleaned = assembler._clean_email(email_with_quotes)

    assert "I have a new idea" in cleaned
    assert "What do you think" in cleaned
    assert "On Jan 5, Bob wrote:" not in cleaned
    assert "Let's meet Tuesday" not in cleaned  # Quoted part removed

def test_signature_removal():
    assembler = ContextAssembler()

    email_with_sig = """Here's my response.

--
Best regards,
Alice Johnson
Senior Manager"""

    cleaned = assembler._clean_email(email_with_sig)

    assert "Here's my response" in cleaned
    assert "Best regards" not in cleaned
    assert "Alice Johnson" not in cleaned

def test_thread_deduplication():
    assembler = ContextAssembler()

    # Simulate thread where Email 2 quotes Email 1
    chunks = [
        {"text": "Let's meet Tuesday", "meta": {"date": "2025-01-15 09:00"}},
        {"text": "> Let's meet Tuesday\nI agree!", "meta": {"date": "2025-01-15 09:15"}},
    ]

    # After cleaning, second email should only have "I agree!"
    unique = assembler._deduplicate_content(chunks)

    assert len(unique) == 2
    assert "Let's meet Tuesday" not in unique[1]["text"]
    assert "I agree!" in unique[1]["text"]
```

**Deliverables:**
- âœ… `scripts/retrieval/context_assembler.py`
- âœ… Quote/reply removal (regex patterns)
- âœ… Signature stripping (common patterns)
- âœ… Thread chronological merging
- âœ… Source attribution formatting
- âœ… Deduplication logic
- âœ… Unit tests (10+ test cases)
- âŒ Skip: Summarization (Phase 4)
- âŒ Skip: Re-ranking (Phase 4)

**Why Context Assembler in Phase 1?**
**Without this, thread retrieval is useless.** Emails are 70% redundant due to quoting. This is the most critical missing piece from the original plan.

---

### Phase 2: Core Retrieval Strategies (Week 1-2) - 8-10 hours

**Priority: ğŸ”´ CRITICAL**

#### 2.1 Thread Retriever with Context Assembly (3-4 hours)

```python
class ThreadRetriever:
    """Retrieves complete email threads with deduplication."""

    def __init__(self, project: ProjectManager):
        self.project = project
        self.retrieval_manager = RetrievalManager(project)
        self.context_assembler = ContextAssembler()

    def retrieve(self, query: str, top_threads: int = 2) -> List[Chunk]:
        """
        Retrieve complete email threads.

        Args:
            query: User query
            top_threads: Number of complete threads to return

        Returns:
            List of chunks representing complete threads, cleaned and ordered
        """
        # Stage 1: Find seed emails using semantic search
        seed_emails = self.retrieval_manager.retrieve(
            query=query,
            top_k=10,
            doc_type="outlook_eml"
        )

        # Stage 2: Group by normalized subject
        threads = self._group_by_thread(seed_emails)

        # Stage 3: Score threads by relevance
        scored_threads = self._score_threads(threads, seed_emails)

        # Stage 4: Get complete threads (expand to full conversations)
        complete_threads = []
        for thread_id in scored_threads[:top_threads]:
            thread_emails = self._get_full_thread(thread_id)
            complete_threads.extend(thread_emails)

        # Stage 5: Sort chronologically
        complete_threads.sort(key=lambda c: c.meta.get("date", ""))

        return complete_threads

    def _get_full_thread(self, thread_id: str) -> List[Chunk]:
        """
        Get ALL emails in a thread, not just those in seed set.

        This ensures complete conversation context.
        """
        # Search for all emails with this normalized subject
        # This might require a metadata filter on the retrieval manager
        all_chunks = self.retrieval_manager.get_all_chunks(
            doc_type="outlook_eml"
        )

        thread_chunks = [
            c for c in all_chunks
            if self._normalize_subject(c.meta.get("subject", "")) == thread_id
        ]

        return thread_chunks

    def _score_threads(self, threads: dict, seed_emails: List[Chunk]) -> List[str]:
        """
        Score threads by relevance.

        Factors:
        - How many seed emails from this thread (higher = more relevant)
        - Thread size (moderate size preferred, not too small/large)
        - Recency of thread
        """
        thread_scores = {}

        for thread_id, chunks in threads.items():
            # Count seed emails in this thread
            seed_count = sum(1 for c in chunks if c in seed_emails)

            # Thread size score (prefer 3-15 emails)
            size = len(chunks)
            size_score = min(size / 15.0, 1.0) if size >= 3 else size / 3.0

            # Recency score (most recent email in thread)
            if chunks:
                most_recent = max(chunks, key=lambda c: c.meta.get("date", ""))
                # Simple recency: could parse date properly
                recency_score = 0.5  # Placeholder
            else:
                recency_score = 0.0

            # Combined score
            thread_scores[thread_id] = (
                seed_count * 2.0 +  # Relevance is most important
                size_score +
                recency_score
            )

        # Return thread IDs sorted by score
        return sorted(thread_scores.keys(),
                     key=lambda tid: thread_scores[tid],
                     reverse=True)
```

**Tests:**
```python
def test_complete_thread_retrieval():
    retriever = ThreadRetriever(project)
    chunks = retriever.retrieve("Budget discussion", top_threads=1)

    # Verify thread completeness
    subjects = [c.meta["subject"] for c in chunks]
    normalized = [retriever._normalize_subject(s) for s in subjects]
    assert len(set(normalized)) == 1  # All same thread

    # Verify chronological order
    dates = [c.meta["date"] for c in chunks]
    assert dates == sorted(dates)

    # Verify reasonable size (complete conversation)
    assert len(chunks) >= 3  # At least a few emails in thread
```

**Deliverables:**
- âœ… `scripts/retrieval/email_thread_retriever.py`
- âœ… Complete thread expansion (not just top-K)
- âœ… Thread scoring by relevance
- âœ… Integration with Context Assembler
- âœ… Unit tests

---

#### 2.2 Temporal Retriever (2-3 hours)

```python
class TemporalRetriever:
    """Retrieves emails filtered by time range."""

    def __init__(self, project: ProjectManager):
        self.project = project
        self.retrieval_manager = RetrievalManager(project)

    def retrieve(self, query: str, intent_metadata: dict, top_k: int = 15) -> List[Chunk]:
        """
        Retrieve emails in specific time range.

        Args:
            query: User query
            intent_metadata: {"time_range": "last_week"} from intent detector
            top_k: Number of chunks to return

        Returns:
            Chronologically sorted chunks from time range
        """
        # Parse time range
        time_range = self.parse_time_range(
            intent_metadata.get("time_range", "recent")
        )

        # Get all email chunks
        all_chunks = self.retrieval_manager.retrieve(
            query=query,
            top_k=100,  # Get more, will filter by date
            doc_type="outlook_eml"
        )

        # Filter by date
        filtered = [
            c for c in all_chunks
            if self._is_in_range(c.meta.get("date", ""), time_range)
        ]

        # Take top K by relevance
        filtered = filtered[:top_k]

        # Sort by date (most recent first)
        filtered.sort(key=lambda c: c.meta.get("date", ""), reverse=True)

        return filtered

    def parse_time_range(self, time_expr: str) -> dict:
        """
        Parse time expression to date range.

        Examples:
            "last_week" â†’ {"start": "2025-11-13", "end": "2025-11-20"}
            "yesterday" â†’ {"start": "2025-11-19", "end": "2025-11-19"}
            "recent" â†’ last 7 days
        """
        from datetime import datetime, timedelta

        now = datetime.now()

        if time_expr == "yesterday":
            start = end = (now - timedelta(days=1)).strftime("%Y-%m-%d")
        elif time_expr == "last_week":
            start = (now - timedelta(days=7)).strftime("%Y-%m-%d")
            end = now.strftime("%Y-%m-%d")
        elif time_expr == "last_month":
            start = (now - timedelta(days=30)).strftime("%Y-%m-%d")
            end = now.strftime("%Y-%m-%d")
        elif time_expr == "this_week":
            # Start of week (Monday)
            start = (now - timedelta(days=now.weekday())).strftime("%Y-%m-%d")
            end = now.strftime("%Y-%m-%d")
        else:  # "recent" or unknown
            start = (now - timedelta(days=7)).strftime("%Y-%m-%d")
            end = now.strftime("%Y-%m-%d")

        return {"start": start, "end": end}

    def _is_in_range(self, date_str: str, time_range: dict) -> bool:
        """Check if date is in range."""
        if not date_str:
            return False

        try:
            # Parse date (format: "2025-01-15 09:30:00")
            date = datetime.strptime(date_str.split()[0], "%Y-%m-%d")
            start = datetime.strptime(time_range["start"], "%Y-%m-%d")
            end = datetime.strptime(time_range["end"], "%Y-%m-%d")

            return start <= date <= end
        except:
            return False
```

**Deliverables:**
- âœ… `scripts/retrieval/email_temporal_retriever.py`
- âœ… Flexible time range parsing
- âœ… Date filtering logic
- âœ… Chronological sorting
- âœ… Unit tests

---

#### 2.3 Sender Retriever (2 hours)

```python
class SenderRetriever:
    """Retrieves emails from specific sender with fuzzy matching."""

    def __init__(self, project: ProjectManager):
        self.project = project
        self.retrieval_manager = RetrievalManager(project)

    def retrieve(self, query: str, intent_metadata: dict, top_k: int = 10) -> List[Chunk]:
        """
        Retrieve emails from specific sender.

        Args:
            query: User query
            intent_metadata: {"sender": "Alice"} from intent detector
            top_k: Number of chunks to return
        """
        sender_name = intent_metadata.get("sender")
        if not sender_name:
            # Fall back to standard retrieval
            return self.retrieval_manager.retrieve(query, top_k=top_k)

        # Get all email chunks
        all_chunks = self.retrieval_manager.retrieve(
            query=query,
            top_k=100,
            doc_type="outlook_eml"
        )

        # Filter by sender (fuzzy match)
        filtered = [
            c for c in all_chunks
            if self._sender_matches(c.meta, sender_name)
        ]

        return filtered[:top_k]

    def _sender_matches(self, meta: dict, query_name: str) -> bool:
        """
        Fuzzy match sender name.

        Examples:
            "Alice" matches:
            - sender_name: "Alice Johnson"
            - sender_name: "Alice J"
            - sender: "alice.j@company.com"
        """
        query_lower = query_name.lower()

        # Check sender_name field
        sender_name = meta.get("sender_name", "").lower()
        if query_lower in sender_name:
            return True

        # Check sender email
        sender_email = meta.get("sender", "").lower()
        if query_lower in sender_email:
            return True

        # Check if first name matches
        if sender_name:
            first_name = sender_name.split()[0] if ' ' in sender_name else sender_name
            if query_lower == first_name:
                return True

        return False
```

**Deliverables:**
- âœ… `scripts/retrieval/email_sender_retriever.py`
- âœ… Fuzzy name matching
- âœ… Email address matching
- âœ… Unit tests

---

#### 2.4 Strategy Selector with Combined Strategies (1-2 hours)

```python
class EmailStrategySelector:
    """Selects retrieval strategy based on intent, supports combined strategies."""

    STRATEGY_MAP = {
        "thread_summary": "thread_retrieval",
        "sender_query": "sender_retrieval",
        "temporal_query": "temporal_retrieval",
        "action_items": "action_extraction",
        "decision_tracking": "decision_extraction",
        "factual_lookup": "adaptive_retrieval"
    }

    def select_strategy(self, intent: dict) -> dict:
        """
        Select strategy based on intent, can return combined strategy.

        Args:
            intent: Output from IntentDetector

        Returns:
            {
                "primary": "sender_retrieval",
                "filters": ["temporal"],  # Apply temporal filter to results
                "params": {...}  # Additional params
            }
        """
        primary_intent = intent["primary_intent"]
        confidence = intent["confidence"]
        secondary_signals = intent.get("secondary_signals", [])
        metadata = intent.get("metadata", {})

        # Low confidence â†’ safe default
        if confidence < 0.5:
            return {"primary": "adaptive_retrieval", "filters": []}

        # Get primary strategy
        primary_strategy = self.STRATEGY_MAP.get(
            primary_intent,
            "adaptive_retrieval"
        )

        # Check for combined strategy needs
        filters = []

        # If temporal signal exists, apply temporal filter
        if "temporal_query" in secondary_signals or "time_range" in metadata:
            if primary_strategy != "temporal_retrieval":
                filters.append("temporal")

        # If sender signal exists, apply sender filter
        if "sender_query" in secondary_signals or "sender" in metadata:
            if primary_strategy != "sender_retrieval":
                filters.append("sender")

        return {
            "primary": primary_strategy,
            "filters": filters,
            "params": metadata
        }
```

**Example:**
```python
# Query: "What did Alice say about budget last week?"
intent = {
    "primary_intent": "sender_query",
    "confidence": 0.9,
    "metadata": {"sender": "Alice", "time_range": "last_week"},
    "secondary_signals": ["temporal_query"]
}

strategy = selector.select_strategy(intent)
# Returns: {
#     "primary": "sender_retrieval",
#     "filters": ["temporal"],  # Apply time filter to Alice's emails
#     "params": {"sender": "Alice", "time_range": "last_week"}
# }
```

**Deliverables:**
- âœ… `scripts/agents/email_strategy_selector.py`
- âœ… Combined strategy support
- âœ… Metadata passing
- âœ… Unit tests

---

### Phase 3: Orchestrator Integration (Week 2) - 6-8 hours

**Priority: ğŸŸ¡ HIGH**

#### 3.1 Email Orchestrator Agent (4-6 hours)

```python
class EmailOrchestratorAgent:
    """
    Main orchestrator for email retrieval and context assembly.

    Coordinates:
    - Intent detection
    - Strategy selection
    - Retrieval execution (including combined strategies)
    - Context assembly and cleaning
    - Logging and debugging
    """

    def __init__(self, project: ProjectManager):
        self.project = project
        self.intent_detector = EmailIntentDetector()
        self.strategy_selector = EmailStrategySelector()
        self.context_assembler = ContextAssembler()

        # Initialize all retrievers
        self.retrievers = {
            "thread_retrieval": ThreadRetriever(project),
            "temporal_retrieval": TemporalRetriever(project),
            "sender_retrieval": SenderRetriever(project),
            "adaptive_retrieval": AdaptiveRetriever(project)
        }

        self.logger = LoggerManager.get_logger("email_orchestrator")

    def retrieve(self, query: str) -> dict:
        """
        Main retrieval orchestration with context assembly.

        Returns:
            {
                "chunks": [...],  # Cleaned, organized chunks
                "context": "...",  # Assembled context string
                "intent": {...},  # Detected intent
                "strategy": {...},  # Strategy used
                "metadata": {...}  # Additional info
            }
        """
        # Step 1: Detect intent
        intent = self.intent_detector.detect(query)
        self._log_intent(intent)

        # Step 2: Select strategy (can be combined)
        strategy = self.strategy_selector.select_strategy(intent)
        self._log_strategy(strategy)

        # Step 3: Execute retrieval
        chunks = self._execute_retrieval(query, strategy)

        # Step 4: Assemble clean context
        context = self.context_assembler.assemble(chunks, intent)

        # Step 5: Build metadata for transparency
        metadata = {
            "chunk_count": len(chunks),
            "strategy_used": strategy["primary"],
            "filters_applied": strategy.get("filters", []),
            "date_range": self._get_date_range(chunks),
            "unique_senders": self._get_unique_senders(chunks)
        }

        return {
            "chunks": chunks,
            "context": context,
            "intent": intent,
            "strategy": strategy,
            "metadata": metadata
        }

    def _execute_retrieval(self, query: str, strategy: dict) -> List[Chunk]:
        """
        Execute retrieval strategy, handling combined strategies.
        """
        # Get primary retriever
        primary_strategy = strategy["primary"]
        retriever = self.retrievers[primary_strategy]

        # Execute primary retrieval
        params = strategy.get("params", {})
        chunks = retriever.retrieve(query, intent_metadata=params)

        # Apply filters if needed
        for filter_type in strategy.get("filters", []):
            if filter_type == "temporal":
                chunks = self._apply_temporal_filter(chunks, params)
            elif filter_type == "sender":
                chunks = self._apply_sender_filter(chunks, params)

        return chunks

    def _apply_temporal_filter(self, chunks: List[Chunk], params: dict) -> List[Chunk]:
        """Apply temporal filter to existing chunks."""
        temporal_retriever = self.retrievers["temporal_retrieval"]
        time_range = temporal_retriever.parse_time_range(
            params.get("time_range", "recent")
        )

        filtered = [
            c for c in chunks
            if temporal_retriever._is_in_range(c.meta.get("date", ""), time_range)
        ]

        return filtered

    def _apply_sender_filter(self, chunks: List[Chunk], params: dict) -> List[Chunk]:
        """Apply sender filter to existing chunks."""
        sender_retriever = self.retrievers["sender_retrieval"]
        sender_name = params.get("sender")

        if not sender_name:
            return chunks

        filtered = [
            c for c in chunks
            if sender_retriever._sender_matches(c.meta, sender_name)
        ]

        return filtered

    def _log_intent(self, intent: dict):
        """Log detected intent."""
        self.logger.info(
            f"Intent detected: {intent['primary_intent']} "
            f"(confidence: {intent['confidence']:.2f})",
            extra={"intent": intent}
        )

    def _log_strategy(self, strategy: dict):
        """Log selected strategy."""
        filters_str = f" + filters: {strategy['filters']}" if strategy.get('filters') else ""
        self.logger.info(
            f"Strategy selected: {strategy['primary']}{filters_str}",
            extra={"strategy": strategy}
        )
```

**Deliverables:**
- âœ… `scripts/agents/email_orchestrator.py`
- âœ… Combined strategy execution
- âœ… Context assembler integration
- âœ… Comprehensive logging
- âœ… Unit tests

---

#### 3.2 Integration with Ask Interface (2 hours)

```python
# Update ask_interface.py to use orchestrator

def run_ask_email_optimized(
    project_path: str,
    query: str,
    model_name: str = "gpt-4o",
    temperature: float = 0.7,
    max_tokens: int = 500,
) -> Tuple[str, List[str], dict]:
    """
    Execute RAG ask pipeline with email optimization.

    Returns:
        answer (str): Generated answer
        sources (List[str]): Cited sources
        metadata (dict): Orchestrator metadata (strategy, intent, etc.)
    """
    project = ProjectManager(project_path)

    # Check if project has email content
    has_emails = _has_email_content(project)

    if has_emails:
        # Use email orchestrator
        orchestrator = EmailOrchestratorAgent(project)
        result = orchestrator.retrieve(query)

        chunks = result["chunks"]
        context = result["context"]  # Pre-assembled clean context
        metadata = result["metadata"]

        # Build prompt with assembled context
        # Note: Don't use PromptBuilder.build_prompt() since context is already formatted
        prompt = EMAIL_PROMPT_TEMPLATE.format(
            context_str=context,
            query_str=query
        )
    else:
        # Standard document retrieval
        retriever = RetrievalManager(project)
        chunks = retriever.retrieve(query=query, top_k=5)

        prompt_builder = PromptBuilder()
        prompt = prompt_builder.build_prompt(query=query, context_chunks=chunks)
        metadata = {}

    # Get answer from LLM
    completer = OpenAICompleter(model_name=model_name)
    answer = completer.get_completion(
        prompt=prompt,
        temperature=temperature,
        max_tokens=max_tokens
    )

    # Build sources list
    sources = []
    for chunk in chunks:
        source = chunk.meta.get("source_filepath", chunk.doc_id)
        if chunk.meta.get("doc_type") == "outlook_eml":
            sender = chunk.meta.get("sender_name", "Unknown")
            subject = chunk.meta.get("subject", "No Subject")
            date = chunk.meta.get("date", "")
            source = f"{sender} - \"{subject}\" ({date})"
        else:
            page = chunk.meta.get("page_number")
            if page:
                source += f", page {page}"
        sources.append(source)

    return answer, sources, metadata
```

**Deliverables:**
- âœ… Updated `scripts/interface/ask_interface.py`
- âœ… Email orchestrator integration
- âœ… Backward compatibility with documents
- âœ… Source formatting for emails
- âœ… Integration tests

---

### Phase 4: Advanced Quality Enhancements (Week 3) - 10-12 hours

**Priority: ğŸŸ¢ MEDIUM (Quality improvements)**

#### 4.1 LLM-Assisted Intent Detection (2-3 hours)

```python
class EmailIntentDetector:
    """Enhanced with optional LLM fallback."""

    def __init__(self, use_llm_fallback: bool = True):
        self.use_llm_fallback = use_llm_fallback
        self.confidence_threshold = 0.6  # Use LLM if below this
        # ... existing patterns ...

    def detect(self, query: str) -> dict:
        """Detect intent with optional LLM fallback."""
        # Try pattern-based first
        result = self._pattern_based_detection(query)

        # If low confidence and LLM enabled, get second opinion
        if (self.use_llm_fallback and
            result["confidence"] < self.confidence_threshold):

            llm_result = self._llm_based_detection(query)

            # Use LLM result if it has higher confidence
            if llm_result["confidence"] > result["confidence"]:
                result = llm_result
                result["detection_method"] = "llm"
            else:
                result["detection_method"] = "pattern_with_llm_check"
        else:
            result["detection_method"] = "pattern"

        return result

    def _llm_based_detection(self, query: str) -> dict:
        """Use LLM to classify intent."""
        prompt = f"""Classify this email query into one of these intents:
- thread_summary: User wants to summarize an email thread or discussion
- sender_query: User wants emails from a specific person
- temporal_query: User wants recent/time-based emails
- action_items: User wants tasks or deadlines
- decision_tracking: User wants decisions made
- factual_lookup: User wants specific information

Also extract metadata:
- sender: name if mentioned
- time_range: if temporal words used (recent, last week, etc.)

Query: "{query}"

Return JSON:
{{
  "intent": "...",
  "confidence": 0.0-1.0,
  "metadata": {{"sender": "...", "time_range": "..."}}
}}
"""
        # Call GPT-3.5-turbo (cheap, fast)
        from scripts.api_clients.openai.completer import OpenAICompleter
        completer = OpenAICompleter(model_name="gpt-3.5-turbo")

        response = completer.get_completion(
            prompt=prompt,
            temperature=0.0,
            max_tokens=100
        )

        # Parse JSON response
        import json
        try:
            result = json.loads(response)
            return {
                "primary_intent": result["intent"],
                "confidence": result["confidence"],
                "metadata": result.get("metadata", {}),
                "secondary_signals": []
            }
        except:
            # Fallback if parsing fails
            return {
                "primary_intent": "factual_lookup",
                "confidence": 0.3,
                "metadata": {},
                "secondary_signals": []
            }
```

**Cost Analysis:**
- GPT-3.5-turbo: ~$0.001 per query (only for ambiguous cases)
- If 20% of queries need LLM: $0.0002 average per query
- Very affordable!

**Deliverables:**
- âœ… LLM fallback in `EmailIntentDetector`
- âœ… Confidence threshold tuning
- âœ… Cost tracking
- âœ… A/B test: pattern vs LLM accuracy

---

#### 4.2 Answer Validator (3-4 hours)

```python
class AnswerValidator:
    """
    Validates LLM answers for quality and accuracy.

    Checks:
    - Required elements present (e.g., list for action items)
    - Citations supported by context
    - Contradictions in source material
    - Completeness
    """

    def __init__(self, use_llm: bool = False):
        self.use_llm = use_llm

    def validate(self, answer: str, context: str, intent: dict) -> dict:
        """
        Validate answer quality.

        Returns:
            {
                "is_valid": True/False,
                "issues": ["contradiction found", ...],
                "suggestions": ["Add conflicting info from Email #3"],
                "confidence": 0.85
            }
        """
        issues = []
        suggestions = []

        # Check 1: Required format based on intent
        format_check = self._check_format(answer, intent)
        if not format_check["valid"]:
            issues.append(format_check["issue"])
            suggestions.append(format_check["suggestion"])

        # Check 2: Contradiction detection
        if self.use_llm:
            contradiction_check = self._check_contradictions_llm(answer, context)
            if contradiction_check["has_contradiction"]:
                issues.append("Potential contradiction detected")
                suggestions.append(contradiction_check["detail"])

        # Check 3: Unsupported claims (simple heuristic)
        unsupported = self._check_unsupported_claims(answer, context)
        if unsupported:
            issues.append(f"Found {len(unsupported)} unsupported claims")
            suggestions.extend(unsupported)

        is_valid = len(issues) == 0
        confidence = 1.0 - (len(issues) * 0.2)  # Rough confidence score

        return {
            "is_valid": is_valid,
            "issues": issues,
            "suggestions": suggestions,
            "confidence": max(confidence, 0.0)
        }

    def _check_format(self, answer: str, intent: dict) -> dict:
        """Check if answer format matches intent."""
        primary_intent = intent.get("primary_intent")

        if primary_intent == "action_items":
            # Should have list or bullets
            has_list = any(marker in answer for marker in ['â€¢', '-', '*', '1.', '2.'])
            if not has_list:
                return {
                    "valid": False,
                    "issue": "Action items query but no list format",
                    "suggestion": "Format answer as bulleted list of tasks"
                }

        elif primary_intent == "sender_query":
            # Should mention the sender's name
            sender = intent.get("metadata", {}).get("sender")
            if sender and sender.lower() not in answer.lower():
                return {
                    "valid": False,
                    "issue": f"Sender '{sender}' not mentioned in answer",
                    "suggestion": f"Ensure answer references what {sender} said"
                }

        return {"valid": True}

    def _check_contradictions_llm(self, answer: str, context: str) -> dict:
        """Use LLM to detect contradictions."""
        prompt = f"""Check if the answer contains contradictory information from the emails.

Emails:
{context}

Answer:
{answer}

Are there any contradictions or conflicting information? If yes, describe them.

Return JSON:
{{
  "has_contradiction": true/false,
  "detail": "Description of contradiction if found"
}}
"""
        from scripts.api_clients.openai.completer import OpenAICompleter
        completer = OpenAICompleter(model_name="gpt-3.5-turbo")

        response = completer.get_completion(prompt=prompt, temperature=0.0, max_tokens=150)

        try:
            import json
            result = json.loads(response)
            return result
        except:
            return {"has_contradiction": False, "detail": ""}

    def _check_unsupported_claims(self, answer: str, context: str) -> List[str]:
        """
        Simple heuristic check for unsupported claims.

        Looks for specific numbers, dates, names in answer that don't appear in context.
        """
        unsupported = []

        # Extract numbers from answer (dates, amounts, etc.)
        import re
        numbers_in_answer = set(re.findall(r'\b\d+\b', answer))
        numbers_in_context = set(re.findall(r'\b\d+\b', context))

        unsupported_numbers = numbers_in_answer - numbers_in_context
        if unsupported_numbers:
            unsupported.append(
                f"Numbers not in context: {', '.join(unsupported_numbers)}"
            )

        # Could add more checks: proper nouns, dates, etc.

        return unsupported
```

**Usage:**
```python
# After LLM generates answer
validator = AnswerValidator(use_llm=True)
validation = validator.validate(
    answer=answer,
    context=assembled_context,
    intent=detected_intent
)

if not validation["is_valid"]:
    logger.warning(f"Answer validation issues: {validation['issues']}")
    # Could either:
    # 1. Regenerate answer with suggestions
    # 2. Append disclaimer to answer
    # 3. Flag for human review
```

**Deliverables:**
- âœ… `scripts/agents/answer_validator.py`
- âœ… Format checking
- âœ… LLM-based contradiction detection
- âœ… Unsupported claim detection
- âœ… Integration tests

---

#### 4.3 Action Item & Decision Extractors (3-4 hours)

```python
class ActionItemExtractor:
    """Extracts action items and deadlines from emails."""

    def __init__(self):
        # Action item patterns
        self.action_patterns = [
            r'(?:TODO|Action item):\s*(.+)',
            r'(?:need to|should|must)\s+(.+?)(?:\.|,|;)',
            r'(?:please|can you)\s+(.+?)(?:\.|,|;)',
            r'(?:by|before|due)\s+(\w+\s+\d+)',  # Deadlines
        ]

    def extract(self, emails: List[Chunk]) -> List[dict]:
        """
        Extract action items from emails.

        Returns:
            [
                {
                    "task": "Review the budget proposal",
                    "deadline": "Friday",
                    "assigned_to": "Bob",
                    "source": "Email from Alice, Jan 15"
                },
                ...
            ]
        """
        # Use LLM for better extraction
        context = self._format_emails(emails)

        prompt = f"""Extract all action items, tasks, and deadlines from these emails.

Emails:
{context}

For each action item, identify:
- The task description
- Deadline (if mentioned)
- Who is responsible (if mentioned)
- Which email it came from

Return as JSON list:
[
  {{
    "task": "Review proposal",
    "deadline": "Friday",
    "assigned_to": "Bob",
    "source_email": "#2"
  }},
  ...
]
"""
        from scripts.api_clients.openai.completer import OpenAICompleter
        completer = OpenAICompleter(model_name="gpt-4o")

        response = completer.get_completion(prompt=prompt, temperature=0.0, max_tokens=500)

        try:
            import json
            action_items = json.loads(response)
            return action_items
        except:
            # Fallback to pattern matching
            return self._extract_with_patterns(emails)

    def _format_emails(self, emails: List[Chunk]) -> str:
        """Format emails for LLM prompt."""
        formatted = []
        for i, email in enumerate(emails):
            sender = email.meta.get("sender_name", "Unknown")
            date = email.meta.get("date", "")
            text = email.text[:500]  # Truncate long emails

            formatted.append(f"Email #{i+1} from {sender} ({date}):\n{text}")

        return "\n\n".join(formatted)

    def _extract_with_patterns(self, emails: List[Chunk]) -> List[dict]:
        """Fallback pattern-based extraction."""
        actions = []

        for email in emails:
            for pattern in self.action_patterns:
                matches = re.findall(pattern, email.text, re.I)
                for match in matches:
                    actions.append({
                        "task": match,
                        "deadline": None,
                        "assigned_to": None,
                        "source": email.meta.get("sender_name", "Unknown")
                    })

        return actions


class DecisionExtractor:
    """Extracts decisions and conclusions from emails."""

    def extract(self, emails: List[Chunk]) -> List[dict]:
        """
        Extract decisions from emails.

        Returns:
            [
                {
                    "decision": "Approved budget of $50K",
                    "made_by": "Sarah",
                    "date": "2025-01-15",
                    "source": "Email #3"
                },
                ...
            ]
        """
        context = self._format_emails(emails)

        prompt = f"""Extract all decisions, conclusions, and approvals from these emails.

Emails:
{context}

For each decision, identify:
- What was decided
- Who made the decision
- When (date)
- Which email

Return as JSON list:
[
  {{
    "decision": "Approved $50K budget",
    "made_by": "Sarah",
    "date": "2025-01-15",
    "source_email": "#3"
  }},
  ...
]
"""
        from scripts.api_clients.openai.completer import OpenAICompleter
        completer = OpenAICompleter(model_name="gpt-4o")

        response = completer.get_completion(prompt=prompt, temperature=0.0, max_tokens=500)

        try:
            import json
            decisions = json.loads(response)
            return decisions
        except:
            return []
```

**Integration with Orchestrator:**
```python
# In EmailOrchestratorAgent

def retrieve(self, query: str) -> dict:
    # ... existing code ...

    # If intent is action_items or decision_tracking, use extractors
    if intent["primary_intent"] == "action_items":
        extractor = ActionItemExtractor()
        action_items = extractor.extract(chunks)

        # Build context from extracted items
        context = "Action Items:\n"
        for item in action_items:
            context += f"- {item['task']}"
            if item.get('deadline'):
                context += f" (due: {item['deadline']})"
            if item.get('assigned_to'):
                context += f" [assigned to: {item['assigned_to']}]"
            context += f" (from {item['source']})\n"

    elif intent["primary_intent"] == "decision_tracking":
        extractor = DecisionExtractor()
        decisions = extractor.extract(chunks)

        # Build context from decisions
        context = "Decisions Made:\n"
        for dec in decisions:
            context += f"- {dec['decision']} (by {dec['made_by']}, {dec['date']})\n"

    else:
        # Standard context assembly
        context = self.context_assembler.assemble(chunks, intent)

    # ... rest of method ...
```

**Deliverables:**
- âœ… `scripts/agents/action_item_extractor.py`
- âœ… `scripts/agents/decision_extractor.py`
- âœ… LLM-based extraction
- âœ… Pattern-based fallback
- âœ… Integration with orchestrator
- âœ… Tests with sample emails

---

#### 4.4 Advanced Context Features (2-3 hours)

**Summarization for Long Threads:**
```python
class ContextAssembler:
    # ... existing code ...

    def assemble(self, chunks: List[Chunk], intent: dict, max_tokens: int = 8000) -> str:
        """Enhanced with summarization for long threads."""
        # ... existing assembly logic ...

        # Check if context is too long
        estimated_tokens = len(context_parts) * 150  # Rough estimate

        if estimated_tokens > max_tokens:
            # Summarize less relevant parts
            context = self._summarize_long_context(context_parts, max_tokens)
        else:
            context = "\n\n---\n\n".join(context_parts)

        return context

    def _summarize_long_context(self, context_parts: List[str], max_tokens: int) -> str:
        """Summarize parts of long threads to fit token limit."""
        # Strategy: Keep first and last emails full, summarize middle
        if len(context_parts) <= 3:
            return "\n\n---\n\n".join(context_parts)

        # Keep first 2 and last 2 full
        kept_full = context_parts[:2] + context_parts[-2:]
        to_summarize = context_parts[2:-2]

        if not to_summarize:
            return "\n\n---\n\n".join(kept_full)

        # Summarize middle emails
        middle_text = "\n\n".join(to_summarize)

        prompt = f"""Summarize these intermediate emails in 2-3 sentences, preserving key points:

{middle_text}

Summary:"""

        from scripts.api_clients.openai.completer import OpenAICompleter
        completer = OpenAICompleter(model_name="gpt-3.5-turbo")
        summary = completer.get_completion(prompt=prompt, temperature=0.3, max_tokens=150)

        # Assemble final context
        final_parts = [
            context_parts[0],
            context_parts[1],
            f"[... {len(to_summarize)} intermediate emails summarized: {summary} ...]",
            context_parts[-2],
            context_parts[-1]
        ]

        return "\n\n---\n\n".join(final_parts)
```

**Re-ranking with Cross-Encoder:**
```python
class ContextAssembler:
    # ... existing code ...

    def __init__(self, use_reranking: bool = False):
        self.use_reranking = use_reranking
        if use_reranking:
            # Load cross-encoder model
            from sentence_transformers import CrossEncoder
            self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

    def assemble(self, chunks: List[Chunk], intent: dict) -> str:
        # ... existing code ...

        # Optionally re-rank chunks before assembly
        if self.use_reranking and len(chunks) > 5:
            chunks = self._rerank_chunks(chunks, intent.get("query", ""))

        # ... rest of assembly ...

    def _rerank_chunks(self, chunks: List[Chunk], query: str) -> List[Chunk]:
        """Re-rank chunks using cross-encoder for better relevance."""
        if not query:
            return chunks

        # Prepare pairs for cross-encoder
        pairs = [(query, chunk.text) for chunk in chunks]

        # Get scores
        scores = self.reranker.predict(pairs)

        # Sort chunks by score
        ranked = sorted(zip(chunks, scores), key=lambda x: x[1], reverse=True)

        return [chunk for chunk, score in ranked]
```

**Noise Filtering:**
```python
class ContextAssembler:
    # ... existing code ...

    NOISE_PATTERNS = {
        "newsletter": [r"unsubscribe", r"newsletter", r"mailing list"],
        "auto_reply": [r"out of office", r"automatic reply", r"away message"],
        "notification": [r"notification", r"alert", r"reminder"]
    }

    def assemble(self, chunks: List[Chunk], intent: dict) -> str:
        # ... existing code ...

        # Filter noise before cleaning
        chunks = self._filter_noise(chunks)

        # ... rest of assembly ...

    def _filter_noise(self, chunks: List[Chunk]) -> List[Chunk]:
        """Filter out newsletters, auto-replies, system notifications."""
        filtered = []

        for chunk in chunks:
            # Check sender domain
            sender = chunk.meta.get("sender", "")
            if any(domain in sender.lower() for domain in ["noreply", "donotreply", "notifications"]):
                continue  # Skip system emails

            # Check content for noise patterns
            text_lower = chunk.text.lower()
            is_noise = False

            for noise_type, patterns in self.NOISE_PATTERNS.items():
                if any(pattern in text_lower for pattern in patterns):
                    is_noise = True
                    break

            if not is_noise:
                filtered.append(chunk)

        return filtered
```

**Deliverables:**
- âœ… Summarization for long threads
- âœ… Cross-encoder re-ranking (optional)
- âœ… Noise filtering (newsletters, auto-replies)
- âœ… Token budget management
- âœ… Tests

---

### Phase 5: UI Integration & Testing (Week 4) - 4-6 hours

**Priority: ğŸŸ¢ MEDIUM**

#### 5.1 UI Updates

**Show Strategy Selection:**
```python
# In ui_v3.py or ask interface

def render_email_query_debug_info(metadata: dict):
    """Show orchestrator decision-making for transparency."""

    with st.expander("ğŸ” Email Strategy Details"):
        col1, col2 = st.columns(2)

        with col1:
            st.write("**Intent Detected:**")
            st.write(f"- Primary: `{metadata['intent']['primary_intent']}`")
            st.write(f"- Confidence: {metadata['intent']['confidence']:.2f}")
            if metadata['intent'].get('secondary_signals'):
                st.write(f"- Secondary: {metadata['intent']['secondary_signals']}")

        with col2:
            st.write("**Strategy Used:**")
            st.write(f"- Primary: `{metadata['strategy_used']}`")
            if metadata.get('filters_applied'):
                st.write(f"- Filters: {metadata['filters_applied']}")

        st.write("**Retrieved Content:**")
        st.write(f"- {metadata['chunk_count']} email chunks")
        st.write(f"- Date range: {metadata['date_range']}")
        st.write(f"- Unique senders: {metadata['unique_senders']}")
```

**Deliverables:**
- âœ… Debug panel showing strategy selection
- âœ… Intent confidence display
- âœ… Retrieved email metadata
- âœ… Answer validation results (if available)

---

#### 5.2 End-to-End Testing

**Test Queries:**
```python
COMPREHENSIVE_TEST_QUERIES = [
    # Thread summary
    "Summarize the Primo NDE discussion",
    "What was the conversation about CSS customization?",

    # Sender queries
    "What did Manuela say about facets?",
    "Alice's opinion on the budget?",

    # Temporal queries
    "Recent emails about migration",
    "What happened last week regarding Primo?",

    # Combined queries (multi-aspect)
    "What did Sarah say about the project last week?",
    "Recent discussions from the IT team about security",

    # Action items
    "What are the action items from the migration thread?",
    "Any deadlines mentioned in recent emails?",

    # Decisions
    "What was decided about the vendor selection?",
    "Final decision on the budget?",

    # Factual lookups
    "What is the budget for Q1?",
    "When is the migration scheduled?"
]
```

**Quality Metrics:**
```python
def evaluate_answer_quality(query: str, answer: str, ground_truth: str = None) -> dict:
    """
    Evaluate answer quality.

    Metrics:
    - Completeness: Does it answer the full question?
    - Accuracy: Is information correct?
    - Citations: Are sources properly cited?
    - Formatting: Is it well-structured?
    """
    metrics = {
        "completeness": 0.0,
        "accuracy": 0.0,
        "has_citations": False,
        "formatting_score": 0.0
    }

    # Check for citations
    metrics["has_citations"] = any(marker in answer for marker in ['[', 'Email', 'from'])

    # Check formatting (bullets, structure)
    has_structure = any(marker in answer for marker in ['\n-', '\n*', '\n1.', '**'])
    metrics["formatting_score"] = 1.0 if has_structure else 0.5

    # If ground truth provided, check accuracy
    if ground_truth:
        # Simple keyword overlap (could use more sophisticated metrics)
        answer_words = set(answer.lower().split())
        truth_words = set(ground_truth.lower().split())
        overlap = len(answer_words & truth_words) / len(truth_words) if truth_words else 0
        metrics["accuracy"] = overlap

    return metrics
```

**Deliverables:**
- âœ… Comprehensive test suite (15+ queries)
- âœ… Quality metrics
- âœ… Comparison with baseline (current K=5 system)
- âœ… Performance benchmarks
- âœ… User acceptance testing

---

## Success Metrics

| Metric | Current (K=5) | Phase 1-2 Target | Phase 3-4 Target | Measurement |
|--------|---------------|------------------|------------------|-------------|
| **Retrieval Quality** |||||
| Thread completeness | 20% (5/25) | 80% | 90% | Manual review of threads |
| Context cleanliness | 30% (70% noise) | 85% | 95% | Quote/signature % removed |
| Temporal accuracy | N/A | 90% | 95% | Date filter correctness |
| Sender precision | N/A | 85% | 90% | Sender match accuracy |
| **Answer Quality** |||||
| Completeness | 60% | 75% | 90% | Expert evaluation |
| Factual accuracy | 70% | 80% | 95% | Verification vs sources |
| Citation quality | 50% | 70% | 85% | Proper source attribution |
| Format appropriateness | 40% | 60% | 85% | Matches query intent |
| **Performance** |||||
| Intent detection | N/A | <50ms | <100ms | Average latency |
| Retrieval time | 500ms | 800ms | 1000ms | Including assembly |
| Total query time | 2s | 2.5s | 3s | End-to-end |
| **User Experience** |||||
| Query success rate | 60% | 80% | 90% | User satisfaction |
| Answer usefulness | 65% | 80% | 90% | User ratings |

---

## File Structure

```
scripts/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ email_intent_detector.py       # Multi-aspect intent detection
â”‚   â”œâ”€â”€ email_strategy_selector.py     # Combined strategy support
â”‚   â”œâ”€â”€ email_orchestrator.py          # Main orchestrator
â”‚   â”œâ”€â”€ action_item_extractor.py       # NEW: Action item extraction
â”‚   â”œâ”€â”€ decision_extractor.py          # NEW: Decision tracking
â”‚   â””â”€â”€ answer_validator.py            # NEW: Answer QA
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ email_thread_retriever.py      # Thread reconstruction
â”‚   â”œâ”€â”€ email_temporal_retriever.py    # Time-based filtering
â”‚   â”œâ”€â”€ email_sender_retriever.py      # Sender-based filtering
â”‚   â”œâ”€â”€ adaptive_retriever.py          # Dynamic K
â”‚   â””â”€â”€ context_assembler.py           # NEW CRITICAL: Context cleaning
â””â”€â”€ interface/
    â””â”€â”€ ask_interface.py                # Updated with orchestrator

tests/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ test_intent_detector.py
â”‚   â”œâ”€â”€ test_orchestrator.py
â”‚   â”œâ”€â”€ test_action_extractor.py
â”‚   â”œâ”€â”€ test_decision_extractor.py
â”‚   â””â”€â”€ test_answer_validator.py
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ test_thread_retriever.py
â”‚   â”œâ”€â”€ test_temporal_retriever.py
â”‚   â”œâ”€â”€ test_sender_retriever.py
â”‚   â””â”€â”€ test_context_assembler.py      # NEW CRITICAL
â””â”€â”€ integration/
    â””â”€â”€ test_email_e2e.py               # End-to-end tests

docs/
â””â”€â”€ EMAIL_AGENTIC_STRATEGY_MERGED.md   # This document
```

---

## Risk Mitigation

### Technical Risks

**1. Context Assembler Complexity**
- Risk: Regex patterns might not catch all quote formats
- Mitigation: Start with common patterns, iterate based on failures
- Fallback: Manual review of cleaned context samples

**2. LLM Cost**
- Risk: LLM-based features (intent, validation, extraction) add cost
- Mitigation:
  - Use GPT-3.5-turbo for cheaper operations
  - Cache frequent patterns
  - Make LLM features optional
- Estimated cost: $0.01-0.02 per query (acceptable)

**3. Performance**
- Risk: More complex pipeline might be slower
- Mitigation:
  - Pattern-based intent is fast (<50ms)
  - Context assembly is local (no API calls)
  - Only LLM steps add latency
- Target: <3s total (acceptable for quality)

**4. Thread Reconstruction Accuracy**
- Risk: Subject normalization might miss threads
- Mitigation:
  - Test with real email data
  - Add fuzzy matching for subjects
  - Fallback to relevance-based grouping

### Implementation Risks

**1. Scope Creep**
- Risk: Too many features, never ship
- Mitigation: **Strict phased approach**
  - Phase 1-2: Must have (Context Assembler, core retrieval)
  - Phase 3: High priority (Orchestrator)
  - Phase 4: Nice to have (Advanced features)
  - Ship after each phase!

**2. Integration Breaking Changes**
- Risk: New components break existing code
- Mitigation:
  - Maintain backward compatibility
  - Feature flags for new features
  - Comprehensive integration tests

---

## Configuration

### Feature Flags

```yaml
# In project config.yml

email_strategy:
  enabled: true

  # Phase 1-2 features (always on)
  use_context_assembler: true
  use_intent_detection: true
  use_combined_strategies: true

  # Phase 4 features (optional)
  use_llm_intent_fallback: false  # Cost consideration
  use_answer_validator: true
  use_reranking: false  # Requires additional library
  use_action_extractors: true
  use_decision_extractors: true

  # Tuning parameters
  context_assembler:
    max_tokens: 8000
    summarize_threshold: 6000
    enable_noise_filtering: true

  intent_detector:
    confidence_threshold: 0.6  # Use LLM if below this

  retrievers:
    thread:
      max_threads: 3
      min_thread_size: 3
    temporal:
      default_range: "last_week"
    adaptive:
      email_k: 15
      document_k: 5
```

---

## Next Steps

### Immediate (This Session)
1. âœ… Create merged implementation plan (this document)
2. ğŸ”„ Review and approve plan
3. ğŸ”„ Begin Phase 1 implementation

### Phase 1 (Next Session) - 8-10 hours
1. Implement Intent Detector with multi-aspect support
2. Implement Context Assembler (quote removal, signatures, dedup) â­ CRITICAL
3. Create comprehensive test suite
4. Test with Primo_List emails

### Phase 2 (Week 1-2) - 8-10 hours
5. Implement Thread Retriever + Context Assembly integration
6. Implement Temporal Retriever
7. Implement Sender Retriever
8. Implement Strategy Selector with combined strategies
9. Test retrieval strategies end-to-end

### Phase 3 (Week 2) - 6-8 hours
10. Implement Email Orchestrator Agent
11. Integrate with ask_interface.py
12. Add UI debug panel
13. End-to-end testing

### Phase 4 (Week 3-4) - 10-12 hours
14. Add LLM intent fallback (optional)
15. Implement Answer Validator
16. Implement Action/Decision Extractors
17. Add summarization and re-ranking
18. Performance optimization

---

## Conclusion

This merged plan combines **agentic orchestration** with **advanced quality enhancements** to create a production-grade email RAG system.

**Critical Success Factors:**
1. **Context Assembler** - Most important missing piece, must be in Phase 1
2. **Phased Approach** - Ship working system quickly, then enhance
3. **Quality Focus** - Answer quality over feature count
4. **Real Testing** - Use your 270 Primo emails to validate each phase

**Key Improvements Over Original Plan:**
- âœ… Context Assembler (quote removal, dedup) - CRITICAL
- âœ… Multi-aspect intent detection
- âœ… Combined retrieval strategies
- âœ… Answer validation layer
- âœ… Specialized extractors (actions, decisions)
- âœ… Production-ready quality controls

**Expected Impact:**
- Thread completeness: **20% â†’ 90%**
- Answer quality: **70% â†’ 90%**
- User satisfaction: **60% â†’ 90%**

The system will deliver **high-quality, context-aware answers** from email data with the sophistication needed for production use.

**Ready to implement Phase 1!** ğŸš€

---

**Last Updated:** 2025-11-20
**Status:** Approved - Ready for Implementation
**Branch:** feature/email-agentic-strategy
**Version:** 2.0 - Merged Plan
