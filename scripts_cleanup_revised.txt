We’re building a structured, deterministic bibliographic engine (MARC→canonical→SQLite→CandidateSet), with optional LLM planning. This means we can remove most RAG-specific code, but we should preserve the infrastructure that gives us:

run orchestration / step execution

per-run artifacts

consistent logging

config loading / validation

The goal is to prune to a lean “pipeline shell”, not to “go back to single scripts with no structure”.

What to REMOVE (agree, with a few caveats)

These are safe to delete because they are tightly coupled to doc-RAG (email/docs/pdf) and vector retrieval:

Remove entirely

agents/

email orchestrators, image insight agents → not needed for MARC POC.

categorization/

document categorization not relevant.

chunking/

MARC is structured; chunking is not core. (We may add FTS later, but not chunking.)

connectors/

Outlook / external connectors → not needed.

embeddings/

delete embedding builders, batching, and embed config. We’re starting with SQLite.

retrieval/

FAISS retrieval + reranking → remove.

tools/ (Outlook deployment / related)

remove.

ui/

Streamlit RAG UI can go for POC (we’ll use CLI outputs + saved artifacts).

Remove MOST of ingestion/ (see “Keep/Repurpose” below)

Remove PDF/DOCX/email loaders.

Keep loader interfaces/registry only if they’re clean and independent.

Remove MOST of prompting/ and interface/

If these are purely “RAG Q&A prompt building”, remove.

But keep a minimal “LLM client + schema validation” layer (see below).

What to KEEP (do not delete)

These are the parts that will save time and avoid rebuilding basics.

Keep all infrastructure

core/ (ProjectManager, config/paths/logging integration)

utils/ keep these explicitly:

logger.py

task_paths.py

config_loader.py

run_logger.py

logger_context.py

Keep the LLM client

api_clients/completer.py (or equivalent)
Keep it as the only LLM dependency for now.

Remove batch_embedder.py and anything embedding-specific.

What to REPURPOSE (important changes)

This is the key improvement over “delete everything RAG”.

1) pipeline/ — do NOT delete; repurpose into MARC workflow runner

Instead of removing pipeline/, convert it to a generic runner for steps like:

parse_marc

normalize_records

build_sqlite_index

compile_query_plan

execute_query_plan

If the current pipeline/ is too RAG-specific, rename it:

pipeline/ → workflow/ (optional), but reusing the step runner is strongly preferred.

Why: you still need deterministic step execution + per-run artifacts.

2) index/ — repurpose to “storage”

Delete FAISS-specific tools, but keep the “index builder” idea and redirect it to SQLite:

index/ becomes the home of:

SQLite schema (schema.sql)

builders (build_index.py)

migrations (optional)
If “index” feels misleading, rename to storage/. Not required, but it reduces confusion.

3) ingestion/ — keep only the abstraction if clean

If ingestion/ has a clean loader interface and registry pattern:

keep BaseLoader / registry

remove everything else

If ingestion/ is deeply tied to chunking and RAG, it’s okay to delete fully and re-create a minimal MARC loader under scripts/marc/.

What to CREATE (new MARC-specific structure)

Your planned new folders are correct. Add two more that will pay off immediately.

Create

scripts/marc/

parser, canonical record extraction, provenance mapping

scripts/normalization/

dates/place/agent normalization (rule-based + confidence)

scripts/query/

QueryPlan schema + compiler + executor

Add schemas/

JSON Schema for QueryPlan, maybe CanonicalRecord later

Add scripts/fixtures/

small MARC XML snippets for unit tests

(Optional but useful) scripts/reports/

coverage reports: parsed fields %, normalization coverage, top unknown date formats, etc.

Important: Don’t delete vector retrieval without replacing “topic search”

Even without embeddings, you can handle “topic X” queries via:

normalized subject headings (6XX)

and later SQLite FTS5 on a fulltext column (245/520/5XX/6XX)

So: Embeddings can be removed safely, but keep in mind we’ll add either:

subject-heading filters (first), then

optional FTS5 (later), then

vectors only if needed.

Execution strategy (safer than “one command”)

Deleting 12 dirs in one command is fast, but it can break imports everywhere. Do this instead:

Phase 0 — checkpoint

Create a git tag/branch checkpoint: pre_cleanup

Phase 1 — remove clearly isolated modules

Remove:

agents/

categorization/

connectors/

tools/

ui/

Run:

python -m pytest (or at least an import smoke test)

python -m compileall or “import core package” check

Phase 2 — remove RAG engine pieces

Remove:

embeddings/

retrieval/

most of chunking/

But only after you’ve verified nothing essential imports them; if imports exist, stub or refactor first.

Phase 3 — repurpose pipeline/index

Keep pipeline/ runner, delete RAG steps

Repurpose index/ into SQLite storage builders

Phase 4 — clean utils + api_clients

remove only the 7 RAG-specific utils

remove batch_embedder.py, keep completer.py

Phase 5 — add new MARC modules + minimal CLI entrypoints

implement scripts/marc/parse.py

implement scripts/index/build_sqlite.py

implement scripts/query/compile.py + schema validation

implement scripts/query/execute.py

update scripts/README.md

Concrete “keep/repurpose/remove” table (for clarity)
KEEP

core/

utils/logger.py, task_paths.py, config_loader.py, run_logger.py, logger_context.py

api_clients/completer.py

pipeline runner skeleton (even if folder renamed)

REPURPOSE

pipeline/ → MARC workflow steps

index/ → SQLite schema + builders (+ optional rename to storage/)

ingestion/ → keep only base loader/registry if clean

REMOVE

agents/

categorization/

chunking/ (most/all)

connectors/

embeddings/

retrieval/

prompting/ (most/all if RAG-only)

interface/ (RAG Q&A; keep a minimal CLI entrypoint elsewhere)

tools/

ui/

ingestion/ implementations (PDF/DOCX/email)

One extra requirement: QueryPlan schema validation

Before you build anything LLM-driven, create:

schemas/query_plan.schema.json

enforce that compile_query() must output valid JSON matching the schema

on failure: retry once with repair prompt, else fail fast

This will prevent “LLM drift” and keep the system deterministic.

Output artifacts rule (must be implemented early)

Every query run should write:

runs/<run_id>/plan.json

runs/<run_id>/sql.txt

runs/<run_id>/candidates.json (record_id + match rationale + evidence fields)

(later) runs/<run_id>/answer.md

This is how we debug and evaluate.