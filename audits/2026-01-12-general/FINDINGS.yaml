# Machine-Readable Audit Findings
# Generated by project-audit skill

metadata:
  audit_date: "2026-01-12"
  project_name: "rare-books-bot"
  auditor: "Claude (project-audit skill)"
  total_findings: 8
  severity_breakdown:
    P0: 0
    P1: 3
    P2: 3
    P3: 2

findings:
  - id: "F001"
    severity: "P1"
    area: "architecture"
    title: "QA Tool Scope Drift - Unclear Project Boundaries"
    description: |
      The ui_qa Streamlit application (12 files, ~1000 lines) represents significant
      scope expansion not clearly documented in project intent. CLAUDE.md mentions
      "acceptance tests" but doesn't describe the QA tool architecture or its
      relationship to the core pipeline. The tool has its own database (qa.db),
      session management, and 5-page UI.

      This creates unclear boundaries: Is ui_qa a development tool or production
      feature? Should it be maintained as part of core or extracted to separate package?
    evidence:
      files:
        - "app/ui_qa/main.py"
        - "app/ui_qa/db.py"
        - "app/ui_qa/pages/*.py"
        - "data/qa/qa.db"
      symbols:
        - "qa_queries"
        - "qa_candidate_labels"
        - "qa_query_gold"
      line_numbers:
        - "app/ui_qa/README.md:1-222"
        - "CLAUDE.md:169-175"
    recommended_invariant: |
      The QA tool's status (development tool vs production feature), scope boundaries,
      and relationship to core pipeline must be explicitly documented in CLAUDE.md
      and README.md.
    acceptance_criteria:
      - "CLAUDE.md includes section on QA tool architecture and scope"
      - "Clear decision documented: keep integrated vs extract to separate package"
      - "If kept: Define which features belong in ui_qa vs core pipeline"
      - "If extracted: Create separate repository and document integration points"
    delegation:
      skill: null
      scope: "Documentation only - no code changes required initially"

  - id: "F002"
    severity: "P1"
    area: "determinism"
    title: "LLM Query Compiler Dependency Without Fallback"
    description: |
      M4 query compilation transitioned from heuristic parser to LLM-based compilation
      (scripts/query/llm_compiler.py). The system now depends on OpenAI API availability
      for all queries. No fallback mechanism exists if:
      1. API is unavailable
      2. API key is missing/invalid
      3. LLM returns invalid JSON (despite schema enforcement)
      4. Query exceeds context limits

      Caching (query_plan_cache.jsonl) mitigates repeat queries but not new queries.
      This violates the "deterministic processing" principle for M4.
    evidence:
      files:
        - "scripts/query/compile.py"
        - "scripts/query/llm_compiler.py"
        - "CLAUDE.md"
      symbols:
        - "compile_query"
        - "compile_query_with_llm"
      line_numbers:
        - "scripts/query/compile.py:36-46"
        - "CLAUDE.md:143-167"
    recommended_invariant: |
      M4 query compilation must either (a) provide fallback mechanism for LLM failures,
      or (b) explicitly document that M4 requires OpenAI API and fail gracefully with
      clear error messages.
    acceptance_criteria:
      - "Add integration tests for LLM failure scenarios (API timeout, invalid key, rate limit)"
      - "Implement graceful failure: clear error message pointing to OPENAI_API_KEY requirement"
      - "Consider: Simple heuristic fallback for basic query patterns (optional)"
      - "Document LLM dependency prominently in README.md and CLI error messages"
    delegation:
      skill: "python-dev-expert"
      scope: "Add error handling to scripts/query/llm_compiler.py and integration tests"

  - id: "F003"
    severity: "P1"
    area: "contracts"
    title: "No Validation of Normalization Confidence Scores"
    description: |
      M2 normalization assigns confidence scores (0.80, 0.95, 0.99) to normalized fields,
      but there's no validation that these scores correlate with actual accuracy. Scores
      are assigned by rule (base cleaning = 0.80, alias map = 0.95) without empirical
      validation.

      Confidence scores may mislead downstream consumers if they don't reflect actual
      accuracy. M4 query execution may trust low-quality normalizations.
    evidence:
      files:
        - "scripts/marc/normalize.py"
        - "scripts/normalization/normalize_agent.py"
        - "CLAUDE.md"
      symbols:
        - "normalize_date_base"
        - "normalize_place_base"
        - "normalize_agent_base"
      line_numbers:
        - "CLAUDE.md:49"
        - "CLAUDE.md:62"
    recommended_invariant: |
      Normalization confidence scores must be validated against ground truth samples,
      or clearly documented as 'heuristic estimates pending validation'.
    acceptance_criteria:
      - "Sample 100-200 normalized records across confidence levels"
      - "Manually validate normalization quality (precision/recall per confidence level)"
      - "Calculate actual accuracy metrics per confidence level"
      - "Either: Update confidence formulas based on empirical data, OR: Document scores as heuristic in CLAUDE.md"
      - "Add regression test suite for normalization quality"
    delegation:
      skill: null
      scope: "Requires manual data labeling and statistical analysis"

  - id: "F004"
    severity: "P2"
    area: "contracts"
    title: "M3 Schema and M4 Query Builder Implicit Coupling"
    description: |
      scripts/query/db_adapter.py hardcodes M3 database table and column names
      without explicit contract. If M3 schema changes, M4 queries could silently break.

      No explicit contract between scripts/marc/m3_schema.sql and scripts/query/db_adapter.py
      makes schema evolution risky.
    evidence:
      files:
        - "scripts/query/db_adapter.py"
        - "scripts/marc/m3_schema.sql"
      symbols:
        - "generate_sql_for_plan"
        - "extract_evidence_from_results"
      line_numbers: []
    recommended_invariant: |
      M3 schema must be explicitly referenced by M4 query builder, with tests
      validating schema expectations.
    acceptance_criteria:
      - "Create scripts/marc/m3_contract.py defining expected tables/columns as constants"
      - "Update db_adapter.py to reference contract constants instead of hardcoded strings"
      - "Add test validating actual database schema matches contract"
      - "Document schema versioning strategy in docs/"
    delegation:
      skill: "python-dev-expert"
      scope: "Refactor db_adapter.py to use explicit schema contract"

  - id: "F005"
    severity: "P2"
    area: "contracts"
    title: "Missing Schema Versioning"
    description: |
      CanonicalRecord, M2EnrichedRecord, and CandidateSet lack version fields.
      Schema evolution would break backward compatibility without migration strategy.

      QueryPlan already has version field ("1.0") but other schemas don't. Adding
      fields or changing schemas would break existing JSONL files and databases.
    evidence:
      files:
        - "scripts/marc/models.py"
        - "scripts/marc/m2_models.py"
        - "scripts/schemas/candidate_set.py"
        - "scripts/schemas/query_plan.py"
      symbols:
        - "CanonicalRecord"
        - "M2EnrichedRecord"
        - "CandidateSet"
        - "QueryPlan"
      line_numbers:
        - "scripts/schemas/query_plan.py:87"
    recommended_invariant: |
      All serialized schemas (CanonicalRecord, M2EnrichedRecord, CandidateSet) must
      include version field and documented migration strategy.
    acceptance_criteria:
      - "Add version: str field to CanonicalRecord, M2EnrichedRecord, CandidateSet"
      - "Create docs/specs/SCHEMA_VERSIONING.md documenting migration strategy"
      - "Implement version checking in parsing logic"
      - "Add tests for version mismatch handling"
    delegation:
      skill: "python-dev-expert"
      scope: "Add version fields to Pydantic models and implement version checking"

  - id: "F006"
    severity: "P2"
    area: "contracts"
    title: "Place Alias Map Lacks Formal Schema"
    description: |
      place_alias_map.json is a critical production artifact but lacks formal schema
      validation. It's a simple dict but no Pydantic model or JSON schema validates
      structure. Malformed alias map could cause silent normalization failures.
    evidence:
      files:
        - "data/normalization/place_aliases/place_alias_map.json"
        - "scripts/marc/m2_normalize.py"
        - "docs/utilities/place_alias_mapping.md"
      symbols:
        - "enrich_with_m2"
      line_numbers: []
    recommended_invariant: |
      place_alias_map.json must be validated against formal schema (Pydantic model
      or JSON schema) during load.
    acceptance_criteria:
      - "Create Pydantic model for alias map: PlaceAliasMap(BaseModel)"
      - "Update m2_normalize.py to validate alias map on load"
      - "Add test validating production alias map passes validation"
      - "Add version field to alias map JSON"
    delegation:
      skill: "python-dev-expert"
      scope: "Create PlaceAliasMap Pydantic model and add validation"

  - id: "F007"
    severity: "P3"
    area: "clarity"
    title: "No Extension Guide for Developers"
    description: |
      Clear extension points exist (add MARC fields, add normalizations, add filter types)
      but no explicit documentation guides developers through extension process.

      Developers must infer patterns from existing code, which increases onboarding
      time and risk of inconsistent implementations.
    evidence:
      files:
        - "docs/"
        - "CLAUDE.md"
      symbols: []
      line_numbers: []
    recommended_invariant: |
      Developers should have clear documentation for common extension scenarios.
    acceptance_criteria:
      - "Create docs/dev_instructions/EXTENSION_GUIDE.md"
      - "Document: Adding new MARC fields, adding normalization types, adding filter types"
      - "Include code examples and test templates"
    delegation:
      skill: null
      scope: "Documentation only"

  - id: "F008"
    severity: "P3"
    area: "tests"
    title: "Test Files for QA Tool Missing"
    description: |
      app/ui_qa/ has 12 Python files but no corresponding test files found in tests/
      directory. Critical paths like database operations (app/ui_qa/db.py) and
      regression runner (app/qa.py) lack test coverage.
    evidence:
      files:
        - "app/ui_qa/db.py"
        - "app/qa.py"
        - "app/ui_qa/pages/*.py"
      symbols:
        - "insert_query"
        - "insert_label"
        - "get_queries"
      line_numbers: []
    recommended_invariant: |
      All non-trivial application code should have corresponding test coverage.
    acceptance_criteria:
      - "Add tests/app/ui_qa/ directory"
      - "Test critical paths: database operations (app/ui_qa/db.py), regression runner (app/qa.py)"
      - "Achieve >80% coverage on non-UI code (exclude Streamlit pages initially)"
    delegation:
      skill: "python-dev-expert"
      scope: "Create test suite for app/ui_qa/db.py and app/qa.py"
